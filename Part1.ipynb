{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Zh9MoN050Ukn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zh9MoN050Ukn",
    "outputId": "f4facc1c-b76e-402b-b196-cf15b2629703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "FuVnav5y1LMf",
   "metadata": {
    "id": "FuVnav5y1LMf"
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# CS B551 Fall 2022, Assignment #4\n",
    "#\n",
    "# Scoring code by D. Crandall\n",
    "#\n",
    "# PLEASE DON'T MODIFY THIS FILE.\n",
    "# Edit pos_solver.py instead!\n",
    "#\n",
    "\n",
    "class Score:\n",
    "    def __init__(self):\n",
    "        self.word_scorecard = {}\n",
    "        self.sentence_scorecard = {}\n",
    "        self.word_count = 0\n",
    "        self.sentence_count = 0\n",
    "\n",
    "    def score(self, algo_outputs, gt):\n",
    "        self.word_count += len(gt)\n",
    "        self.sentence_count += 1\n",
    "\n",
    "        for algo, labels in algo_outputs.items():\n",
    "            correct = 0\n",
    "            for j in range(0, len(gt)):\n",
    "                correct += 1 if gt[j] == labels[j] else 0\n",
    "\n",
    "            self.word_scorecard[algo] = self.word_scorecard.get(algo, 0) + correct\n",
    "            self.sentence_scorecard[algo] = self.sentence_scorecard.get(algo, 0) + (correct == len(gt))\n",
    "\n",
    "    def print_scores(self):\n",
    "        print(\"\\n==> So far scored %d sentences with %d words.\" % (self.sentence_count, self.word_count))\n",
    "        print(\"                   Words correct:     Sentences correct: \")\n",
    "\n",
    "        for i in sorted(self.word_scorecard):\n",
    "            print(\"%18s:     %7.2f%%             %7.2f%%\" % (i, self.word_scorecard[i] * 100 / float(self.word_count),\n",
    "                                                             self.sentence_scorecard[i] * 100 / float(\n",
    "                                                                 self.sentence_count)))\n",
    "\n",
    "    @staticmethod\n",
    "    def print_helper(description, list, sentence):\n",
    "        print((\"%40s\" % description) + \" \" + \" \".join(\n",
    "            [((\"%-\" + str(max(5, len(sentence[i]))) + \"s\") % list[i]) for i in range(0, len(list))]))\n",
    "\n",
    "    @staticmethod\n",
    "    def print_results(sentence, outputs, posteriors, models):\n",
    "        Score.print_helper(\" \".join([(\"%7s\" % model) for model in models]), sentence, sentence)\n",
    "        for algo in sorted(outputs.keys()):\n",
    "            Score.print_helper(algo + \"  \" + \" \".join(\n",
    "                [(\"%7.2f\" % posteriors[algo][model]) if algo in posteriors else \" \" * 7 for model in models]),\n",
    "                               outputs[algo], sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b8a4ae6e-b138-44a9-8454-2cf87b96fcec",
   "metadata": {
    "id": "b8a4ae6e-b138-44a9-8454-2cf87b96fcec"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import product\n",
    "import random\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "class Solver:\n",
    "    \"\"\"\n",
    "    This class predicts parts-of-speech for words in a sentence\n",
    "    in three ways using different bayesian network architectures:\n",
    "        1. a Simple model using only emission probabilities\n",
    "            for parts-of-speech given a word\n",
    "        2. a Hidden Markov model implemented by Viterbi algorithm\n",
    "        3. a Complex bayesian model by Monte Carlo simulations\n",
    "            implemented by Gibbs sampling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, default_prob=1e-6):\n",
    "        \"\"\"\n",
    "        These dictionaries store the log probabilities\n",
    "        corresponding to different edges in the bayesian models:\n",
    "            1. Emissions: POS --> Word\n",
    "            2. Transitions: POS --> next POS\n",
    "            3. Emission2s: POS --> next Word\n",
    "            4. Transition2s: POs --> next next POS\n",
    "        The default part-of-speech probability is also defined here\n",
    "        for words which are not in the training data.\n",
    "        \"\"\"\n",
    "        self.emissions = {}  # simple\n",
    "        self.transitions = {}  # hmm\n",
    "        self.emission2s = {}  # complex\n",
    "        self.transition2s = {}  # complex\n",
    "        self.default_prob = math.log(1e-6)\n",
    "\n",
    "    def train(self, data: list, verbose=False):\n",
    "        \"\"\"\n",
    "        This method fills the log probability dictionaries for the\n",
    "        emissions, transition, emission2s, and transition2s\n",
    "        bayesian network edges.\n",
    "        It takes a list of tuples composed of two objects each:\n",
    "            1. a tuple of words, representing the sentence\n",
    "            2. a tuple of parts-of-speech, corresponding to\n",
    "                each word in the sentence\n",
    "        The occurence of each edge is counted and normalized\n",
    "        and LaPlace smoothing is applied for non-existent edges.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"training...\")\n",
    "            print(\"counting emissions and transitions\")\n",
    "        # Initialize counters for emissions and transitions\n",
    "        emission_counts = defaultdict(int)\n",
    "        transition_counts = defaultdict(int)\n",
    "        tag_counts = defaultdict(int)\n",
    "        start_tag_counts = defaultdict(int)\n",
    "        emission2_counts = defaultdict(int)\n",
    "        transition2_counts = defaultdict(int)\n",
    "\n",
    "        # Count emissions and transitions from the training data\n",
    "        for sentence, tags in data:\n",
    "            start_tag_counts[tags[0]] += 1  # Increment the start tag count\n",
    "            # count emission from current tag\n",
    "            for i in range(len(tags)):\n",
    "                tag = tags[i]\n",
    "                word = sentence[i].lower()\n",
    "                tag_counts[tag] += 1\n",
    "                emission_counts[(tag, word)] += 1  # simple\n",
    "                # count transition and emission from previous tag\n",
    "                if i > 0:\n",
    "                    prev_tag = tags[i - 1]\n",
    "                    transition_counts[(prev_tag, tag)] += 1  # hmm\n",
    "                    emission2_counts[(prev_tag, word)] += 1  # complex\n",
    "                # count transition from previous previous tag\n",
    "                if i > 1:\n",
    "                    prev2_tag = tags[i - 2]\n",
    "                    transition2_counts[(prev2_tag, tag)] += 1  # complex\n",
    "\n",
    "        # Get sets of all words and tags to determine vocab and number of tags\n",
    "        self.word_list = list(set(word for sentence, _ in data\n",
    "                                  for word in sentence))\n",
    "        self.tag_list = list(set(tag for _, tags in data\n",
    "                                 for tag in tags))\n",
    "\n",
    "        # Laplace smoothing - add one to all counts\n",
    "        if verbose:\n",
    "            print(\"Laplace smoothing - Add one to all counts\")\n",
    "        for tag in self.tag_list:\n",
    "            start_tag_counts[tag] += 1\n",
    "        for tag in self.tag_list:\n",
    "            for word in self.word_list:\n",
    "                emission_counts[(tag, word)] += 1\n",
    "                emission2_counts[(tag, word)] += 1\n",
    "        for prev_tag in self.tag_list:\n",
    "            for tag in self.tag_list:\n",
    "                transition_counts[(prev_tag, tag)] += 1\n",
    "                transition2_counts[(prev_tag, tag)] += 1\n",
    "\n",
    "        # Convert counts to probabilities with smoothing\n",
    "        # Use total_words for emission probabilities\n",
    "        if verbose:\n",
    "            print(\"computing probabilities...\")\n",
    "        self.initial = {\n",
    "            k: math.log(v / (tag_counts[k] + len(self.tag_list)))\n",
    "            for k, v in start_tag_counts.items()\n",
    "        }\n",
    "        self.emissions = {\n",
    "            k: math.log(v / (tag_counts[k[0]] + len(self.word_list)))\n",
    "            for k, v in emission_counts.items()\n",
    "        }\n",
    "        self.transitions = {\n",
    "            k: math.log(v / (tag_counts[k[0]] + len(self.tag_list)))\n",
    "            for k, v in transition_counts.items()\n",
    "        }\n",
    "        self.emission2s = {\n",
    "            k: math.log(v / (tag_counts[k[0]] + len(self.word_list)))\n",
    "            for k, v in emission2_counts.items()\n",
    "        }\n",
    "        self.transition2s = {\n",
    "            k: math.log(v / (tag_counts[k[0]] + len(self.tag_list)))\n",
    "            for k, v in transition2_counts.items()\n",
    "        }\n",
    "        if verbose:\n",
    "            print(\"training complete!\")\n",
    "\n",
    "    def posterior(self, model: str, sentence: tuple, label: tuple):\n",
    "        \"\"\"\n",
    "        This method computes the posterior probabilities of a given sentence\n",
    "        as labeled by a corresponding set of parts-of-speech.\n",
    "        The method is implemented for each different model,\n",
    "        Simple, HMM, and MCMC.\n",
    "        \"\"\"\n",
    "        if model == \"Simple\":\n",
    "            # calculate the sum of emission log probabilities\n",
    "            probability_sum = 0\n",
    "            for word, tag in zip(sentence, label):\n",
    "                probability_sum += self.emissions.get((tag, word.lower()),\n",
    "                                                      self.default_prob)\n",
    "            return probability_sum\n",
    "\n",
    "        elif model == \"HMM\":\n",
    "            # For the HMM model, calculate the total log probability\n",
    "            total_log_prob = 0\n",
    "            for i in range(len(sentence)):\n",
    "                word = sentence[i].lower()\n",
    "                tag = label[i]\n",
    "                # Emission probability\n",
    "                emission_prob = self.emissions.get((tag, word),\n",
    "                                                   self.default_prob)\n",
    "                total_log_prob += emission_prob\n",
    "                # transition probability\n",
    "                if i == 0:\n",
    "                    # Transition from start tag\n",
    "                    start_transition_prob = self.initial.get(tag,\n",
    "                                                             self.default_prob)\n",
    "                    total_log_prob += start_transition_prob\n",
    "                else:\n",
    "                    # Transition from previous tag\n",
    "                    prev_tag = label[i-1]\n",
    "                    transition_prob = self.transitions.get((prev_tag, tag),\n",
    "                                                           self.default_prob)\n",
    "                    total_log_prob += transition_prob\n",
    "            return total_log_prob\n",
    "\n",
    "        elif model == \"Complex\":\n",
    "            # For the complex model, calculate the total log probability\n",
    "            total_log_prob = 0\n",
    "            for i in range(len(sentence)):\n",
    "                word = sentence[i].lower()\n",
    "                tag = label[i]\n",
    "                # emission probability to current word\n",
    "                emission_prob = self.emissions.get((tag, word),\n",
    "                                                   self.default_prob)\n",
    "                total_log_prob += emission_prob\n",
    "                # transition from start tag\n",
    "                if i == 0:\n",
    "                    start_transition_prob = self.initial.get(tag,\n",
    "                                                             self.default_prob)\n",
    "                    total_log_prob += start_transition_prob\n",
    "                # transition from previous tag\n",
    "                else:\n",
    "                    prev_tag = label[i-1]\n",
    "                    transition_prob = self.transitions.get((prev_tag, tag),\n",
    "                                                           self.default_prob)\n",
    "                    total_log_prob += transition_prob\n",
    "                # emission to next word\n",
    "                if i != range(len(sentence))[-1]:\n",
    "                    next_word = sentence[i+1].lower()\n",
    "                    emission2_prob = self.emission2s.get((tag, next_word),\n",
    "                                                         self.default_prob)\n",
    "                    total_log_prob += emission2_prob\n",
    "                # transition from previous previous tag\n",
    "                if i > 1:\n",
    "                    prev2_tag = label[i-2]\n",
    "                    transition2_prob = self.transition2s.get((prev2_tag, tag),\n",
    "                                                             self.default_prob)\n",
    "                    total_log_prob += transition2_prob\n",
    "            return total_log_prob\n",
    "        else:\n",
    "            print(\"Unknown algorithm!\")\n",
    "\n",
    "    def simplified(self, sentence: tuple):\n",
    "        \"\"\"\n",
    "        Simplified POS tagging method assigns the most probable tag\n",
    "        to each word as the argmax of the emission log probability\n",
    "        \"\"\"\n",
    "        argmax_list = [\n",
    "            max(self.tag_list,\n",
    "                key=lambda tag: self.emissions.get((tag, word.lower()), 0))\n",
    "            for word in sentence\n",
    "        ]\n",
    "        print(\"Simple Model complete!\")\n",
    "        return argmax_list\n",
    "\n",
    "    def hmm_viterbi(self, sentence: tuple):\n",
    "        \"\"\"\n",
    "        Implementation of the Viterbi algorithm for HMM mode\n",
    "        \"\"\"\n",
    "        # Number of tags\n",
    "        n_tags = len(self.tag_list)\n",
    "        # Initialize the Viterbi matrix with negative infinity\n",
    "        viterbi = [[-math.inf for _ in range(n_tags)]\n",
    "                   for _ in range(len(sentence))]\n",
    "        # Backpointer matrix to track the best path\n",
    "        backpointer = [[0 for _ in range(n_tags)]\n",
    "                       for _ in range(len(sentence))]\n",
    "\n",
    "        # Initialization step using log probabilities\n",
    "        first_word = sentence[0].lower()\n",
    "        for tag_idx, tag in enumerate(self.tag_list):\n",
    "            # Check for the presence of (word, tag) in emission probabilities\n",
    "            if (tag, first_word) in self.emissions:\n",
    "                viterbi[0][tag_idx] = self.initial.get(tag,\n",
    "                                                       self.default_prob) + \\\n",
    "                                        self.emissions.get((tag, first_word),\n",
    "                                                           self.default_prob)\n",
    "            else:\n",
    "                # Handle unseen (word, tag) pairs\n",
    "                viterbi[0][tag_idx] = self.initial.get(tag,\n",
    "                                                       self.default_prob) + \\\n",
    "                                        self.default_prob\n",
    "\n",
    "        # Iterate through the rest of the sentence\n",
    "        for t in range(1, len(sentence)):\n",
    "            word = sentence[t].lower()\n",
    "            for tag_idx, tag in enumerate(self.tag_list):\n",
    "                # Find the max log probability and corresponding tag index\n",
    "                # from the previous step\n",
    "                max_log_prob, best_tag_idx = max([\n",
    "                    (\n",
    "                        viterbi[t-1][prevtag_idx] +\n",
    "                        self.transitions.get((self.tag_list[prevtag_idx], tag),\n",
    "                                             self.default_prob),\n",
    "                        prevtag_idx\n",
    "                    )\n",
    "                    for prevtag_idx in range(n_tags)\n",
    "                ], key=lambda x: x[0])\n",
    "\n",
    "                # Update the Viterbi matrix\n",
    "                if (tag, word) in self.emissions:\n",
    "                    viterbi[t][tag_idx] = max_log_prob + \\\n",
    "                                          self.emissions.get((tag, word),\n",
    "                                                             self.default_prob)\n",
    "                # Handle unseen pairs\n",
    "                else:\n",
    "                    viterbi[t][tag_idx] = max_log_prob + self.default_prob\n",
    "\n",
    "                # Update the backpointer matrix\n",
    "                backpointer[t][tag_idx] = best_tag_idx\n",
    "\n",
    "        # Termination step\n",
    "        max_prob, best_tag_idx = max(\n",
    "            [(viterbi[len(sentence)-1][tag_idx], tag_idx)\n",
    "             for tag_idx in range(n_tags)],\n",
    "            key=lambda x: x[0]\n",
    "        )\n",
    "\n",
    "        # Backtrack to find the best path\n",
    "        best_path = [best_tag_idx]\n",
    "        for t in range(len(sentence)-1, 0, -1):\n",
    "            best_path.insert(0, backpointer[t][best_path[0]])\n",
    "\n",
    "        print(\"Hidden Markov Model complete!\")\n",
    "        # Convert numeric indices to tags\n",
    "        return [self.tag_list[i] for i in best_path]\n",
    "\n",
    "    def SampleKeys(self, edge_probs: dict, n: int):\n",
    "        \"\"\"\n",
    "        This method gets random keys (bayesian network edges)\n",
    "        as given by their values (probabilities).\n",
    "        This is the mechanism that implements the Monte Carlo simulations.\n",
    "        \"\"\"\n",
    "        edges = list(edge_probs.keys())\n",
    "        probabilities = [math.exp(v) for v in edge_probs.values()]\n",
    "        sampled_edges = random.choices(edges, probabilities, k=n)\n",
    "        return sampled_edges\n",
    "\n",
    "    def mcmc_word_probs(self, sentence: tuple, predicted_pos: list, i: int):\n",
    "        \"\"\"\n",
    "        This method gets all possible POS log probabilities\n",
    "        for the observed word given its context in the sentence.\n",
    "        \"\"\"\n",
    "        word_probs = {}\n",
    "        word_probs['emission'] = {\n",
    "            k: v for k, v in self.emissions.items()\n",
    "            if k[1] == sentence[i]\n",
    "        }\n",
    "        if i == 0:\n",
    "            word_probs['initial'] = {\n",
    "                k: v for k, v in self.initial.items()\n",
    "            }\n",
    "        if i > 0:\n",
    "            word_probs['transition'] = {\n",
    "                k: v for k, v in self.transitions.items()\n",
    "                if k[0] == predicted_pos[-1]\n",
    "            }\n",
    "        if i != range(len(sentence))[-1]:\n",
    "            word_probs['emission2'] = {\n",
    "                k: v for k, v in self.emission2s.items()\n",
    "                if k[1] == sentence[i+1]\n",
    "            }\n",
    "        if i > 1:\n",
    "            word_probs['transition2'] = {\n",
    "                k: v for k, v in self.transition2s.items()\n",
    "                if k[0] == predicted_pos[-2]\n",
    "            }\n",
    "        # if its a new word, fill probabilities with default log probability\n",
    "        for k, v in word_probs.items():\n",
    "            if k == 'initial' and len(v) == 0:\n",
    "                word_probs[k] = {\n",
    "                    tag: self.default_prob for tag in self.tag_list\n",
    "                }\n",
    "            elif k == 'emission' and len(v) == 0:\n",
    "                word_probs[k] = {\n",
    "                    (tag, sentence[i]): self.default_prob\n",
    "                    for tag in self.tag_list\n",
    "                }\n",
    "            elif k == 'transition' and len(v) == 0:\n",
    "                word_probs[k] = {\n",
    "                    (predicted_pos[-1], tag): self.default_prob\n",
    "                    for tag in self.tag_list\n",
    "                }\n",
    "            elif k == 'emission2' and len(v) == 0:\n",
    "                word_probs[k] = {\n",
    "                    (tag, sentence[i+1]): self.default_prob\n",
    "                    for tag in self.tag_list\n",
    "                }\n",
    "            elif k == 'transition2' and len(v) == 0:\n",
    "                word_probs[k] = {\n",
    "                    (predicted_pos[-2], tag): self.default_prob\n",
    "                    for tag in self.tag_list\n",
    "                }\n",
    "        return word_probs\n",
    "\n",
    "    def complex_mcmc(self, sentence: tuple, n=100):\n",
    "        \"\"\"\n",
    "        This method applies Gibbs sampling to perform Markov Chain Monte Carlo\n",
    "        simulations to estimate the distribution of the Complex model\n",
    "        and predict the parts of speech.\n",
    "        \"\"\"\n",
    "        print(f\"Monte Carlo Markov Chain with {n} simulations per word...\")\n",
    "        # iterate through each word in the sentence to get predicted POS\n",
    "        predicted_pos = []\n",
    "        for i in trange(len(sentence)):\n",
    "            # first get the relevant possible probabilities of POS for the word\n",
    "            word_probs = self.mcmc_word_probs(sentence=sentence,\n",
    "                                              predicted_pos=predicted_pos,\n",
    "                                              i=i)\n",
    "            # then sample from the probabilities to get the most frequent POS\n",
    "            pos_samples = []\n",
    "            # special case for 1st character in 1-character sentence\n",
    "            if i == 0 and i == range(len(sentence))[-1]:\n",
    "                combinations = product(word_probs['initial'].keys(),\n",
    "                                       word_probs['emission'].keys())\n",
    "                initial_probs = {\n",
    "                    emk: sum([word_probs['initial'][ink],\n",
    "                              word_probs['emission'][emk]])\n",
    "                    for ink, emk in combinations\n",
    "                    if ink == emk[0]\n",
    "                }\n",
    "                pos_samples.extend([pos for pos, word\n",
    "                                    in self.SampleKeys(initial_probs, n=n)])\n",
    "\n",
    "            # draw the first POS from the initial probability distribution\n",
    "            elif i == 0:\n",
    "                combinations = product(word_probs['initial'].keys(),\n",
    "                                       word_probs['emission'].keys(),\n",
    "                                       word_probs['emission2'].keys())\n",
    "                initial_probs = {\n",
    "                    emk: sum([word_probs['initial'][ink],\n",
    "                              word_probs['emission'][emk],\n",
    "                              word_probs['emission2'][em2k]])\n",
    "                    for ink, emk, em2k in combinations\n",
    "                    if ink == emk[0] == em2k[0]\n",
    "                }\n",
    "                pos_samples.extend([pos for pos, word\n",
    "                                    in self.SampleKeys(initial_probs, n=n)])\n",
    "\n",
    "            # special case for 2nd character in 2-character sentence\n",
    "            elif i == 1 and i == range(len(sentence))[-1]:\n",
    "                combinations = product(word_probs['transition'].keys(),\n",
    "                                       word_probs['emission'].keys())\n",
    "                final_probs = {\n",
    "                    emk: sum([word_probs['transition'][trk],\n",
    "                              word_probs['emission'][emk]])\n",
    "                    for trk, emk in combinations\n",
    "                    if trk[1] == emk[0]\n",
    "                }\n",
    "                pos_samples.extend([pos for pos, word\n",
    "                                    in self.SampleKeys(final_probs, n=n)])\n",
    "\n",
    "            # draw the second POS from emission, transition, and emission2\n",
    "            elif i == 1:\n",
    "                combinations = product(word_probs['transition'].keys(),\n",
    "                                       word_probs['emission'].keys(),\n",
    "                                       word_probs['emission2'].keys())\n",
    "                next_probs = {\n",
    "                    emk: sum([word_probs['transition'][trk],\n",
    "                              word_probs['emission'][emk],\n",
    "                              word_probs['emission2'][em2k]])\n",
    "                    for trk, emk, em2k in combinations\n",
    "                    if trk[1] == emk[0] == em2k[0]\n",
    "                }\n",
    "                pos_samples.extend([pos for pos, word\n",
    "                                    in self.SampleKeys(next_probs, n=n)])\n",
    "\n",
    "            # draw POS from emission, transition, emission2, and transition2\n",
    "            elif i != range(len(sentence))[-1]:\n",
    "                combinations = product(word_probs['transition'].keys(),\n",
    "                                       word_probs['emission'].keys(),\n",
    "                                       word_probs['transition2'].keys(),\n",
    "                                       word_probs['emission2'].keys())\n",
    "                next_probs = {\n",
    "                    emk: sum([word_probs['transition'][trk],\n",
    "                              word_probs['emission'][emk],\n",
    "                              word_probs['transition2'][tr2k],\n",
    "                              word_probs['emission2'][em2k]])\n",
    "                    for trk, emk, tr2k, em2k in combinations\n",
    "                    if trk[1] == emk[0] == tr2k[1] == em2k[0]\n",
    "                }\n",
    "                pos_samples.extend([pos for pos, word\n",
    "                                    in self.SampleKeys(next_probs, n=n)])\n",
    "\n",
    "            # draw the final POS from emission, transition, and transition2\n",
    "            else:\n",
    "                combinations = product(word_probs['transition'].keys(),\n",
    "                                       word_probs['emission'].keys(),\n",
    "                                       word_probs['transition2'].keys())\n",
    "                final_probs = {\n",
    "                    emk: sum([word_probs['transition'][trk],\n",
    "                              word_probs['emission'][emk],\n",
    "                              word_probs['transition2'][tr2k]])\n",
    "                    for trk, emk, tr2k in combinations\n",
    "                    if trk[1] == emk[0] == tr2k[1]\n",
    "                }\n",
    "                pos_samples.extend([pos for pos, word\n",
    "                                    in self.SampleKeys(final_probs, n=n)])\n",
    "\n",
    "            # assign part-of-speech as most commonly sampled POS occurence\n",
    "            best_pos = Counter(pos_samples).most_common(1)[0][0]\n",
    "            predicted_pos.append(best_pos)\n",
    "        return predicted_pos\n",
    "\n",
    "    def solve(self, model: str, sentence: tuple):\n",
    "        \"\"\"\n",
    "        This solve() method is called by label.py,\n",
    "        so you should keep the interface the same,\n",
    "        but you can change the code itself.\n",
    "        It should return a list of part-of-speech labelings of the sentence,\n",
    "        one part of speech per word.\n",
    "        \"\"\"\n",
    "        if model == \"Simple\":\n",
    "            return self.simplified(sentence)\n",
    "        elif model == \"HMM\":\n",
    "            return self.hmm_viterbi(sentence)\n",
    "        elif model == \"Complex\":\n",
    "            return self.complex_mcmc(sentence)\n",
    "        else:\n",
    "            print(\"Unknown algorithm!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34caa506-a432-40aa-9d2c-c5323686b683",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66adb617-e0cb-4869-9acb-f399d9c7abc7",
    "outputId": "e7ebc44b-813c-4b59-edf0-95cb3dce9e11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning model...\n",
      "Loading test data...\n",
      "Testing classifiers...\n",
      "Sentence: ('at', 'the', 'same', 'instant', ',', 'nick', 'hit', 'the', 'barrel', 'and', 'threw', 'himself', 'upon', 'the', 'smaller', 'man', '.')\n",
      "Simple Model complete!\n",
      "Hidden Markov Model complete!\n",
      "Monte Carlo sampling the Complex Markov Chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 17/17 [00:07<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Simple     HMM Complex at    the   same  instant ,     nick  hit   the   barrel and   threw himself upon  the   smaller man   .    \n",
      "0. Ground truth   -87.92 -115.81 -245.25 adp   det   adj   noun    .     noun  verb  det   noun   conj  verb  pron    adp   det   adj     noun  .    \n",
      "      1. Simple   -87.22 -120.48 -257.37 adp   det   adj   noun    .     x     verb  det   noun   conj  verb  pron    adp   det   adj     noun  .    \n",
      "         2. HMM   -87.75 -114.89 -245.43 adp   det   adj   noun    .     pron  verb  det   noun   conj  verb  pron    adp   det   adj     noun  .    \n",
      "     3. Complex   -87.92 -115.81 -245.25 adp   det   adj   noun    .     noun  verb  det   noun   conj  verb  pron    adp   det   adj     noun  .    \n",
      "\n",
      "==> So far scored 1 sentences with 17 words.\n",
      "                   Words correct:     Sentences correct: \n",
      "   0. Ground truth:      100.00%              100.00%\n",
      "         1. Simple:       94.12%                0.00%\n",
      "            2. HMM:       94.12%                0.00%\n",
      "        3. Complex:      100.00%              100.00%\n",
      "----\n",
      "Sentence: ('the', 'gun', 'fired', 'next', 'to', 'his', 'ear', 'with', 'a', 'vicious', 'whoosh', 'like', 'the', 'first', 'stroke', 'of', 'an', 'old', 'steam', 'engine', '.')\n",
      "Simple Model complete!\n",
      "Hidden Markov Model complete!\n",
      "Monte Carlo sampling the Complex Markov Chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Simple     HMM Complex the   gun   fired next  to    his   ear   with  a     vicious whoosh like  the   first stroke of    an    old   steam engine .    \n",
      "0. Ground truth  -123.99 -155.46 -334.75 det   noun  verb  adp   adp   det   noun  adp   det   adj     prt    adp   det   adj   noun   adp   det   adj   noun  noun   .    \n",
      "      1. Simple  -120.03 -152.90 -333.52 det   noun  verb  adj   prt   det   noun  adp   det   adj     .      adp   det   adj   noun   adp   det   adj   noun  noun   .    \n",
      "         2. HMM  -121.22 -147.57 -321.80 det   noun  verb  adj   adp   det   noun  adp   det   adj     noun   adp   det   adj   noun   adp   det   adj   noun  noun   .    \n",
      "     3. Complex  -121.22 -147.57 -321.80 det   noun  verb  adj   adp   det   noun  adp   det   adj     noun   adp   det   adj   noun   adp   det   adj   noun  noun   .    \n",
      "\n",
      "==> So far scored 2 sentences with 38 words.\n",
      "                   Words correct:     Sentences correct: \n",
      "   0. Ground truth:      100.00%              100.00%\n",
      "         1. Simple:       89.47%                0.00%\n",
      "            2. HMM:       92.11%                0.00%\n",
      "        3. Complex:       94.74%               50.00%\n",
      "----\n",
      "Sentence: ('at', 'the', 'same', 'instant', ',', 'elaine', 'screamed', 'wildly', ',', 'the', 'sound', 'ending', 'abruptly', 'as', 'nick', 'went', 'off', 'the', 'boat', 'and', 'into', 'the', 'water', 'on', 'top', 'of', 'the', 'frantic', ',', 'struggling', 'poet', '.')\n",
      "Simple Model complete!\n",
      "Hidden Markov Model complete!\n",
      "Monte Carlo sampling the Complex Markov Chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████▌                                           | 8/32 [00:03<00:11,  2.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# run all algorithms on the sentence\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (algo, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(Algorithms, Algorithm_labels):\n\u001b[0;32m---> 79\u001b[0m     outputs[label] \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# calculate posteriors for each output under each model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m posteriors \u001b[38;5;241m=\u001b[39m {o: {a: solver\u001b[38;5;241m.\u001b[39mposterior(a, s, outputs[o]) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m Algorithms} \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs}\n",
      "Cell \u001b[0;32mIn[93], line 476\u001b[0m, in \u001b[0;36mSolver.solve\u001b[0;34m(self, model, sentence)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhmm_viterbi(sentence)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex_mcmc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown algorithm!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[93], line 365\u001b[0m, in \u001b[0;36mSolver.complex_mcmc\u001b[0;34m(self, sentence, n)\u001b[0m\n\u001b[1;32m    362\u001b[0m predicted_pos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mlen\u001b[39m(sentence)):\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# first get the relevant possible probabilities of POS for the word\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     word_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcmc_word_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mpredicted_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicted_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# then sample from the probabilities to get the most frequent POS\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     pos_samples \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[93], line 317\u001b[0m, in \u001b[0;36mSolver.mcmc_word_probs\u001b[0;34m(self, sentence, predicted_pos, i)\u001b[0m\n\u001b[1;32m    312\u001b[0m     word_probs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    313\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransitions\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m predicted_pos[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    315\u001b[0m     }\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence))[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 317\u001b[0m     word_probs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memission2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memission2s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    322\u001b[0m     word_probs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransition2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition2s\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m predicted_pos[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    325\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[93], line 319\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m     word_probs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    313\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransitions\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m predicted_pos[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    315\u001b[0m     }\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence))[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    317\u001b[0m     word_probs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memission2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    318\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memission2s\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    320\u001b[0m     }\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    322\u001b[0m     word_probs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransition2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition2s\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m predicted_pos[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    325\u001b[0m     }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "path = '/home/mark/Nextcloud/Local/Mark/GradSchool/Coursework/CSCI_551/margree-jonjhans-a4/Part1/'\n",
    "\n",
    "def read_data(fname):\n",
    "    exemplars = []\n",
    "    file = open(fname, 'r');\n",
    "    for line in file:\n",
    "        data = tuple([w.lower() for w in line.split()])\n",
    "        exemplars += [(data[0::2], data[1::2]), ]\n",
    "    return exemplars\n",
    "\n",
    "\n",
    "(train_file, test_file) = (path+'bc.train', path+'bc.test')\n",
    "\n",
    "print(\"Learning model...\")\n",
    "solver = Solver()\n",
    "\n",
    "train_data = read_data(train_file)\n",
    "solver.train(train_data)\n",
    "\n",
    "class Score:\n",
    "    def __init__(self):\n",
    "        self.word_scorecard = {}\n",
    "        self.sentence_scorecard = {}\n",
    "        self.word_count = 0\n",
    "        self.sentence_count = 0\n",
    "\n",
    "    def score(self, algo_outputs, gt):\n",
    "        self.word_count += len(gt)\n",
    "        self.sentence_count += 1\n",
    "\n",
    "        for algo, labels in algo_outputs.items():\n",
    "            correct = 0\n",
    "            for j in range(0, len(gt)):\n",
    "                correct += 1 if gt[j] == labels[j] else 0\n",
    "\n",
    "            self.word_scorecard[algo] = self.word_scorecard.get(algo, 0) + correct\n",
    "            self.sentence_scorecard[algo] = self.sentence_scorecard.get(algo, 0) + (correct == len(gt))\n",
    "\n",
    "    def print_scores(self):\n",
    "        print(\"\\n==> So far scored %d sentences with %d words.\" % (self.sentence_count, self.word_count))\n",
    "        print(\"                   Words correct:     Sentences correct: \")\n",
    "\n",
    "        for i in sorted(self.word_scorecard):\n",
    "            print(\"%18s:     %7.2f%%             %7.2f%%\" % (i, self.word_scorecard[i] * 100 / float(self.word_count),\n",
    "                                                             self.sentence_scorecard[i] * 100 / float(\n",
    "                                                                 self.sentence_count)))\n",
    "\n",
    "    @staticmethod\n",
    "    def print_helper(description, list, sentence):\n",
    "        print((\"%40s\" % description) + \" \" + \" \".join(\n",
    "            [((\"%-\" + str(max(5, len(sentence[i]))) + \"s\") % list[i]) for i in range(0, len(list))]))\n",
    "\n",
    "    @staticmethod\n",
    "    def print_results(sentence, outputs, posteriors, models):\n",
    "        Score.print_helper(\" \".join([(\"%7s\" % model) for model in models]), sentence, sentence)\n",
    "        for algo in sorted(outputs.keys()):\n",
    "            Score.print_helper(algo + \"  \" + \" \".join(\n",
    "                [(\"%7.2f\" % posteriors[algo][model]) if algo in posteriors else \" \" * 7 for model in models]),\n",
    "                               outputs[algo], sentence)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_data = read_data(test_file)\n",
    "\n",
    "print(\"Testing classifiers...\")\n",
    "scorer = Score()\n",
    "\n",
    "Algorithms = (\"Simple\", \"HMM\", \"Complex\")\n",
    "Algorithm_labels = [str(i + 1) + \". \" + Algorithms[i] for i in range(0, len(Algorithms))]\n",
    "# for (s, gt) in sorted(test_data, key=lambda x: len(x[0])):\n",
    "for (s, gt) in test_data:\n",
    "    print(\"Sentence:\", s)\n",
    "\n",
    "    outputs = {\"0. Ground truth\": gt}\n",
    "\n",
    "    # run all algorithms on the sentence\n",
    "    for (algo, label) in zip(Algorithms, Algorithm_labels):\n",
    "        outputs[label] = solver.solve(algo, s)\n",
    "\n",
    "        # calculate posteriors for each output under each model\n",
    "    posteriors = {o: {a: solver.posterior(a, s, outputs[o]) for a in Algorithms} for o in outputs}\n",
    "    Score.print_results(s, outputs, posteriors, Algorithms)\n",
    "\n",
    "    scorer.score(outputs, gt)\n",
    "    scorer.print_scores()\n",
    "\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ed50f1-198b-4a41-b269-363429a40ab1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39ed50f1-198b-4a41-b269-363429a40ab1",
    "outputId": "a126671a-8439-4173-bca0-dcbe749b79a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['det', 'noun', 'verb', 'adj', 'adp', 'det', 'noun', 'verb', 'adj']\n"
     ]
    }
   ],
   "source": [
    "sentence = (\"the\", \"cat\", \"is\", \"green\", \"although\", \"it's\", \"eyes\", \"are\", \"blue\")\n",
    "\n",
    "# get all the emission and transition probabilities for each word in the sentence\n",
    "# this is the slowest part of this method.\n",
    "# The emissions items are huge so it takes time to filter them for individual words\n",
    "all_probs = {}\n",
    "for i in range(len(sentence)):\n",
    "    word_probs = {}\n",
    "    word_probs['emission'] = {k: v for k, v in solver.emissions.items()\n",
    "                              if k[1] == sentence[i]}\n",
    "    if i == 0:\n",
    "        word_probs['initial'] = {k: v for k, v in solver.initial.items()}\n",
    "    if i > 0:\n",
    "        word_probs['transition'] = {k: v for k, v in solver.transitions.items()}\n",
    "    if i != range(len(sentence))[-1]:\n",
    "        word_probs['emission2'] = {k: v for k, v in solver.emission2s.items()\n",
    "                                   if k[1] == sentence[i+1]}\n",
    "    if i > 1:\n",
    "        word_probs['transition2'] = {k: v for k, v in solver.transition2s.items()}\n",
    "\n",
    "    all_probs[i] = word_probs\n",
    "# set gibbs sampling iterations\n",
    "n = 2500\n",
    "# initiate dict for storing the samples\n",
    "all_samples = {i: [] for i in range(len(sentence))}\n",
    "# draw probability distribution for POS initial state\n",
    "initial_probs = {}\n",
    "for k1, v1 in all_probs[0]['emission'].items():\n",
    "    for k2, v2 in all_probs[0]['emission2'].items():\n",
    "        if k1[0] == k2[0]:\n",
    "            initial_probs[k1] = v1 * v2 * all_probs[0]['initial'][k1[0]]\n",
    "initial_probs = initial_probs if len(initial_probs) > 0 else {t: 1e-6 for t in solver.tag_list}\n",
    "# for each sample parse the sentence to draw a state\n",
    "# from the complex markov chain\n",
    "for m in range(0, n):\n",
    "    for i in range(len(sentence)):\n",
    "        if i == 0:  # draw the first POS from the emission and the initial\n",
    "            all_samples[i].append(SampleKeys(initial_probs, n=1)[0][0])\n",
    "            continue\n",
    "        # calculate transition, emission, and emission2 probabilities for 2nd+ POS\n",
    "        tr = {k: v for k, v in all_probs[i]['transition'].items()\n",
    "              if k[0] == all_samples[i-1][m]}\n",
    "        tr = tr if len(tr) > 0 else {(all_samples[i-1][m], t): 1e-6 for t in solver.tag_list}\n",
    "\n",
    "        em = {k: v for k, v in all_probs[i]['emission'].items()}\n",
    "        em = em if len(em) > 0 else {(t, sentence[i]): 1e-6 for t in solver.tag_list}\n",
    "\n",
    "        if i != range(len(sentence))[-1]:\n",
    "            em2 = {k: v for k, v in all_probs[i]['emission2'].items()}\n",
    "            em2 = em2 if len(em2) > 0 else {(t, sentence[i+1]): 1e-6 for t in solver.tag_list}\n",
    "        if i == 1:  # draw the second POS from the emission, transition, and emission2\n",
    "            next_prob = {\n",
    "                emk: tr[trk] * em[emk] * em2[em2k]\n",
    "                for trk, emk, em2k in product(tr.keys(), em.keys(), em2.keys())\n",
    "                if trk[1] == emk[0] == em2k[0]\n",
    "            }\n",
    "        elif i != range(len(sentence))[-1]:  # draw the remaining POS from the emission, transition, emission2, and transition2\n",
    "            tr2 = {k: v for k, v in all_probs[i]['transition2'].items()\n",
    "                   if k[0] == all_samples[i-2][m]}\n",
    "            tr2 = tr2 if len(tr2) > 0 else {(all_samples[i-2][m], t): 1e-6 for t in solver.tag_list}\n",
    "            next_prob = {\n",
    "                emk: tr[trk] * em[emk] * tr2[tr2k] * em2[em2k]\n",
    "                for trk, emk, tr2k, em2k in product(tr.keys(), em.keys(), tr2.keys(), em2.keys())\n",
    "                if trk[1] == emk[0] == tr2k[1] == em2k[0]\n",
    "            }\n",
    "        else:  # draw the remaining POS from the emission, transition, emission2, and transition2\n",
    "            tr2 = {k: v for k, v in all_probs[i]['transition2'].items()\n",
    "                   if k[0] == all_samples[i-2][m]}\n",
    "            tr2 = tr2 if len(tr2) > 0 else {(all_samples[i-2][m], t): 1e-6 for t in solver.tag_list}\n",
    "            next_prob = {\n",
    "                emk: tr[trk] * em[emk] * tr2[tr2k]\n",
    "                for trk, emk, tr2k in product(tr.keys(), em.keys(), tr2.keys())\n",
    "                if trk[1] == emk[0] == tr2k[1]\n",
    "            }\n",
    "        all_samples[i].append(SampleKeys(next_prob, n=1)[0][0])\n",
    "# get the POS pattern which was drawn the most - the max marginal\n",
    "max_marginal = list(max(\n",
    "    Counter(map(tuple, zip(*all_samples.values()))).items(),\n",
    "    key=lambda x: x[1]\n",
    ")[0])\n",
    "print(max_marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38863a3b-3661-4e04-a842-5b956d055373",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38863a3b-3661-4e04-a842-5b956d055373",
    "outputId": "da78755f-dc65-4215-f6f3-dae3b83984f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('det', 'the'): 0.034302742430860116,\n",
       " ('x', 'the'): 1.4880481566396015e-06,\n",
       " ('pron', 'the'): 2.241926758097272e-06,\n",
       " ('prt', 'the'): 9.715436000802033e-07,\n",
       " ('adp', 'the'): 2.923559709831986e-07,\n",
       " ('.', 'the'): 1.4385498517437183e-07,\n",
       " ('noun', 'the'): 9.790676110349678e-08,\n",
       " ('adj', 'the'): 2.0711127937399897e-07,\n",
       " ('adv', 'the'): 1.0194848343522371e-06,\n",
       " ('verb', 'the'): 7.313841922004554e-08,\n",
       " ('num', 'the'): 1.074573795135696e-06,\n",
       " ('conj', 'the'): 8.932585012427945e-07}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v * all_probs[0]['initial'][k[0]]\n",
    " for k, v in all_probs[0]['emission'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b11132-7d0e-40fc-9fc1-ba8ff2ecfe47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90b11132-7d0e-40fc-9fc1-ba8ff2ecfe47",
    "outputId": "7692d1de-d33a-4d27-cab4-009a941d2b2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('.', 'is'): 0.003953360356302857,\n",
       " ('det', 'is'): 0.005802986720811165,\n",
       " ('noun', 'is'): 0.016782116092255554,\n",
       " ('verb', 'is'): 0.001028478947505808,\n",
       " ('adv', 'is'): 0.0020910496401837456,\n",
       " ('pron', 'is'): 0.02488804793440555,\n",
       " ('prt', 'is'): 0.008125877225382286,\n",
       " ('conj', 'is'): 0.0019270833333333334,\n",
       " ('num', 'is'): 0.0008833772190605623,\n",
       " ('adp', 'is'): 0.0003013489798450712,\n",
       " ('adj', 'is'): 0.0009404946493480085,\n",
       " ('x', 'is'): 0.00023656422718768146}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=1\n",
    "\n",
    "{k: v for k, v in all_probs[i]['emission2'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4111489-478f-45de-aaf0-9b92e7635014",
   "metadata": {
    "id": "f4111489-478f-45de-aaf0-9b92e7635014",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading test data...\")\n",
    "test_data = read_data(test_file)\n",
    "\n",
    "print(\"Testing classifiers...\")\n",
    "scorer = Score()\n",
    "\n",
    "Algorithms = (\"Simple\", \"HMM\", \"Complex\")\n",
    "Algorithm_labels = [str(i + 1) + \". \" + Algorithms[i] for i in range(0, len(Algorithms))]\n",
    "for (s, gt) in test_data:\n",
    "\n",
    "    outputs = {\"0. Ground truth\": gt}\n",
    "\n",
    "    # run all algorithms on the sentence\n",
    "    for (algo, label) in zip(Algorithms, Algorithm_labels):\n",
    "        outputs[label] = solver.solve(algo, s)\n",
    "\n",
    "        # calculate posteriors for each output under each model\n",
    "    posteriors = {o: {a: solver.posterior(a, s, outputs[o]) for a in Algorithms} for o in outputs}\n",
    "    Score.print_results(s, outputs, posteriors, Algorithms)\n",
    "\n",
    "    scorer.score(outputs, gt)\n",
    "    scorer.print_scores()\n",
    "\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b4674-ebd9-47f7-85d9-f52cb7f16049",
   "metadata": {
    "collapsed": true,
    "id": "310b4674-ebd9-47f7-85d9-f52cb7f16049",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "0a0f0c64-331d-46c2-fb25-8b276f04f1f7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.95 s, sys: 3.26 ms, total: 6.95 s\n",
      "Wall time: 7.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['det', 'noun', 'verb', 'adj', 'adp', 'prt', 'noun', 'verb', 'adj', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "\n",
    "# this function gets random keys as given by the probabilities (values)\n",
    "def SampleKeys(dictionary, n):\n",
    "    keys = list(dictionary.keys())\n",
    "    probabilities = list(dictionary.values())\n",
    "    sampled_keys = random.choices(keys, probabilities, k=n)\n",
    "    return sampled_keys\n",
    "\n",
    "sentence = (\"the\", \"cat\", \"is\", \"black\", \"although\", \"it's\", \"eyes\", \"are\", \"yellow\", \".\")\n",
    "\n",
    "# get all the emission and transition probabilities for each word in the sentence\n",
    "all_probs = {}\n",
    "for i in range(0, len(sentence)):\n",
    "    word = sentence[i]\n",
    "    word_probs = {}\n",
    "    word_probs['emission'] = {k: v for k, v in solver.emissions.items()\n",
    "                              if k[1] == word}\n",
    "    if i == 0:\n",
    "        word_probs['initial'] = {k: v for k, v in solver.initial.items()}\n",
    "    if i > 0:\n",
    "        word_probs['transition'] = {k: v for k, v in solver.transitions.items()}\n",
    "        word_probs['emission2'] = {k: v for k, v in solver.emission2s.items()\n",
    "                                   if k[1] == word}\n",
    "    if i > 1:\n",
    "        word_probs['transition2'] = {k: v for k, v in solver.transition2s.items()}\n",
    "    all_probs[i] = word_probs\n",
    "\n",
    "\n",
    "# set gibbs sampling iterations\n",
    "n = 2000\n",
    "# initiate dict for storing the samples\n",
    "all_samples = {i: [] for i in range(0, len(sentence))}\n",
    "# draw probability distribution for initial state\n",
    "initial_probs = {k: v * all_probs[0]['initial'][k[0]]\n",
    "                 for k, v in all_probs[0]['emission'].items()}\n",
    "# for each sample parse the sentence to draw a state\n",
    "# from the complex markov chain\n",
    "for m in range(0, n):\n",
    "    for i in range(0, len(sentence)):\n",
    "        if i == 0:\n",
    "            all_samples[i].append(SampleKeys(initial_probs, n=1)[0][0])\n",
    "            continue\n",
    "        em2 = [v for k, v in all_probs[i]['emission2'].items()\n",
    "               if k[0] == all_samples[i-1][m]][0]\n",
    "        tr = {k: v for k, v in all_probs[i]['transition'].items()\n",
    "              if k[0] == all_samples[i-1][m]}\n",
    "        em = {k: v for k, v in all_probs[i]['emission'].items()}\n",
    "        if i == 1:\n",
    "            next_prob = {\n",
    "                emk: tr[trk] * em[emk] * em2\n",
    "                for trk, emk in product(tr.keys(), em.keys())\n",
    "                if trk[1] == emk[0]\n",
    "            }\n",
    "        else:\n",
    "            tr2 = {k: v for k, v in all_probs[i]['transition2'].items()\n",
    "                   if k[0] == all_samples[i-2][m]}\n",
    "            next_prob = {\n",
    "                emk: tr[trk] * em[emk] * tr2[tr2k] * em2\n",
    "                for trk, emk, tr2k in product(tr.keys(), em.keys(), tr2.keys())\n",
    "                if trk[1] == emk[0] == tr2k[1]\n",
    "            }\n",
    "        all_samples[i].append(SampleKeys(next_prob, n=1)[0][0])\n",
    "\n",
    "list(max(Counter(map(tuple, zip(*all_samples.values()))).items(),key=lambda x: x[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f2a24-4273-4eb2-8012-197fb2debdb0",
   "metadata": {
    "id": "3f7f2a24-4273-4eb2-8012-197fb2debdb0",
    "outputId": "90c8a0fa-1f90-487a-928d-ddf02ff75dd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['emissions', 'transitions', 'emission2s', 'transition2s', 'word_list', 'tag_list', 'initial'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ced6b-fb5f-42aa-80da-c6b1c4a2833a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
